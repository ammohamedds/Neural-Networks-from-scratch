{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=levekYbxauw&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3&index=8\n",
    "# https://github.com/Sentdex/NNfSiX/blob/master/Python/p008-Categorical-Cross-Entropy-Loss-applied.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create Data\n",
    "#https://cs231n.github.io/neural-networks-case-study/\n",
    "# https://gist.github.com/Sentdex/454cb20ec5acf0e76ee8ab8448e6266c\n",
    "# points = 100 # number of points per class\n",
    "# D = 2 # dimensionality\n",
    "# classes = 3 # number of classes\n",
    "def spiral_data(points, classes):\n",
    "    X = np.zeros((points*classes, 2))  # data matrix  # D = 2 # dimensionality\n",
    "    y = np.zeros(points*classes, dtype='uint8')   #class labels\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points*class_number, points*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, points)  # radius\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n",
      "(300,)\n",
      "300\n",
      "300\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "print(len(X.shape))\n",
    "print(len(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will intial weight randomly, so the dimension of Weights should be matrix(4,N) as the input is matrx(3,4)\n",
    "    \n",
    "class Layer_Dense:\n",
    "    # we need to generate Weights and Biases\n",
    "    def __init__(self, n_inputs, n_neurons): \n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)     # 0.1 to ensure that the range of values is low ( less than 1)\n",
    "        self.biases = np.zeros((1,n_neurons))   # biases we will be added to output of sum of weights so should be matrix(1,N )\n",
    "        \n",
    "    def forward(self,inputs):  # inputs is last ouput\n",
    "        self.output = np.dot(inputs,self.weights)+self.biases\n",
    "\n",
    "        \n",
    "class Activation_Rellu:\n",
    "    def forward(self, inputs): # inputs here is output of summation of wieghts and neurons\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        \n",
    "               \n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "        \n",
    "class Loss:\n",
    "    def calculate(self,output,y):\n",
    "        sample_losses =self.forward(output,y) # will return negative_log_likelihoods from Loss_CategoricalCrossentropy class\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss): # will be inhert from loss class\n",
    "    def forward(self,y_pred,y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_clipped_pred =  np.clip(y_pred, 1e-7, 1-1e-7)  # use clip() to avoid inifinty issue in case of log(0)\n",
    "        # y_true (target) will be passed either in form of scalar values [1,0,1] or One hot encoding [[0 ,1],[1,0],[0,1]]\n",
    "        if len(y_true.shape) == 1: # scalar values, just 1 d\n",
    "            correct_confidences = y_clipped_pred[range(samples), y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2: #One hot encoding, so it will be just muliplication of two matrix, then sum or (max value) to get one value per row or element (not our case in this data)\n",
    "            correct_confidences = np.sum(y_clipped_pred*y_true, axis=1)\n",
    "    \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[1.09861229 1.09865032 1.09866862 1.09872078 1.09866165 1.09861954\n",
      " 1.0988573  1.09868236 1.09862676 1.09862847 1.09865048 1.09904661\n",
      " 1.0986552  1.09866084 1.09866307 1.09866803 1.09867357 1.09867545\n",
      " 1.09868099 1.09868423 1.09868881 1.09868161 1.09869618 1.09865298\n",
      " 1.0986983  1.09869271 1.09869906 1.09868616 1.09871985 1.09866875\n",
      " 1.0986401  1.0986332  1.09867029 1.09862397 1.0986537  1.09865145\n",
      " 1.09867717 1.09863667 1.09862742 1.09865132 1.09862804 1.09891994\n",
      " 1.09863895 1.09893478 1.09863034 1.09948022 1.09879556 1.09954714\n",
      " 1.09950906 1.09905835 1.09863882 1.10023042 1.09874936 1.10069928\n",
      " 1.09898447 1.0998798  1.10053085 1.10075312 1.10090537 1.10095385\n",
      " 1.10107236 1.10111671 1.10061322 1.10117157 1.10094437 1.10106285\n",
      " 1.09905835 1.09956313 1.09899492 1.09894075 1.09934773 1.10145232\n",
      " 1.10076425 1.10073994 1.09887557 1.09914107 1.09890704 1.09891223\n",
      " 1.09891681 1.09882834 1.09892658 1.09892887 1.09892675 1.09877101\n",
      " 1.09883229 1.09884272 1.09885881 1.09888339 1.09893956 1.09879226\n",
      " 1.09887115 1.09895485 1.09888511 1.09873758 1.09871671 1.09861268\n",
      " 1.09874062 1.09879062 1.0986468  1.0986512  1.09861229 1.09857757\n",
      " 1.0986078  1.09856698 1.09861367 1.09855561 1.09861033 1.0984521\n",
      " 1.09854043 1.09840045 1.09829952 1.09861516 1.09841614 1.09829998\n",
      " 1.0982528  1.09810004 1.09833578 1.09810524 1.09795187 1.09791112\n",
      " 1.0979716  1.09783766 1.09780075 1.09798055 1.0977451  1.09769553\n",
      " 1.09769408 1.097694   1.09734645 1.09757108 1.09765142 1.09747872\n",
      " 1.09750936 1.09752295 1.09758175 1.09737473 1.09635008 1.09701261\n",
      " 1.09714118 1.09698325 1.09613402 1.09706455 1.09566386 1.09598862\n",
      " 1.09551267 1.09553235 1.09529191 1.0952985  1.09530835 1.09514481\n",
      " 1.09539335 1.09528267 1.09750637 1.09787775 1.09844788 1.09533394\n",
      " 1.09469582 1.0981942  1.09640223 1.09475752 1.09862682 1.09612444\n",
      " 1.09825839 1.09863076 1.09866296 1.09863267 1.09859533 1.09838766\n",
      " 1.09862836 1.09813837 1.09807973 1.09775992 1.09733749 1.09818889\n",
      " 1.09715611 1.09634001 1.09660655 1.0971118  1.09693613 1.09621299\n",
      " 1.09570255 1.09706443 1.09563714 1.0957128  1.09551547 1.0956995\n",
      " 1.0955326  1.09540044 1.09542796 1.09532784 1.09563891 1.09553531\n",
      " 1.09573998 1.09551208 1.09532874 1.09553051 1.09557037 1.09547201\n",
      " 1.09492981 1.09539295 1.09861229 1.09867753 1.0987115  1.09880796\n",
      " 1.09888306 1.09875419 1.09895911 1.09908584 1.09883295 1.09922252\n",
      " 1.09929379 1.09929237 1.09942757 1.09924562 1.09909262 1.09894703\n",
      " 1.09910551 1.09860292 1.0994982  1.09941403 1.09920001 1.09962742\n",
      " 1.09908926 1.0995326  1.09964306 1.09948537 1.09862146 1.09859754\n",
      " 1.09856884 1.0985404  1.09863975 1.0986407  1.09863195 1.09864218\n",
      " 1.09864284 1.0986289  1.09852206 1.09864622 1.09861141 1.09849163\n",
      " 1.09847455 1.09864987 1.09865053 1.0985095  1.09856763 1.09843752\n",
      " 1.09842612 1.09855387 1.0984184  1.09843312 1.09841395 1.09856147\n",
      " 1.09842878 1.0999724  1.09840574 1.09900237 1.09839263 1.09963834\n",
      " 1.10113366 1.1012488  1.1010398  1.10159684 1.09857785 1.10026755\n",
      " 1.10036356 1.10285088 1.09968271 1.1024506  1.10280038 1.10335553\n",
      " 1.10340366 1.10298663 1.1034194  1.101915   1.10261784 1.10342372\n",
      " 1.10294216 1.09858093 1.10230463 1.10008681 1.10375412 1.10264176\n",
      " 1.10225263 1.09864778 1.10116369 1.09852579 1.09905494 1.09852888\n",
      " 1.09853056 1.10057665 1.0985738  1.0985789  1.09869516 1.09850658\n",
      " 1.09854452 1.09869495 1.09852928 1.09860076 1.09831057 1.09849806]\n",
      "Loss: 1.0985889608867985\n"
     ]
    }
   ],
   "source": [
    "# X, y = spiral_data(samples_batch=100, classes_indata=3)\n",
    "X, y = spiral_data(points=100, classes=3)\n",
    "\n",
    "layer1= Layer_Dense(2,5)  # X has two features(neurons) as input, 5 is my desire neurons\n",
    "Activation1 = Activation_Rellu()\n",
    "\n",
    "layer2= Layer_Dense(5,4) # we choose 5 to be like previous output neurons and 4 is my desire neurons\n",
    "Activation2 = Activation_Rellu()\n",
    "\n",
    "layer3= Layer_Dense(4,3)\n",
    "Activation3 = Activation_Softmax()\n",
    "\n",
    "\n",
    "layer1.forward(X)\n",
    "Activation1.forward(layer1.output)\n",
    "\n",
    "layer2.forward(Activation1.output)\n",
    "Activation2.forward(layer2.output)\n",
    "Activation2.output.shape\n",
    "\n",
    "layer3.forward(Activation2.output)\n",
    "Activation3.forward(layer3.output)\n",
    "\n",
    "# print(Activation3.output.shape)\n",
    "# print(Activation3.output[:5])\n",
    "# print(Activation3.output)\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "print(loss_function.forward(Activation3.output, y).shape)\n",
    "print(loss_function.forward(Activation3.output, y)) # before mean\n",
    "loss = loss_function.calculate(Activation3.output, y)\n",
    "\n",
    "\n",
    "print(\"Loss:\", loss)\n",
    "#  Loss vaues are very high for all values and also mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
