{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "X = [[1,2,3,2.5],[2,5,-1,2],[-1.5,2.7,3.3,-0.8]]  # Input Matrix(3,4)\n",
    "# https://github.com/Sentdex/NNfSiX/blob/master/Python/p004-Layers-and-Object.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will intial weight randomly, so the dimension of Weights should be matrix(4,N) as the input is matrx(3,4)\n",
    "    \n",
    "class Layer_Dense:\n",
    "    # we need to generate Weights and Biases\n",
    "    def __init__(self, n_inputs, n_neurons): \n",
    "        # here as we do transpose for input (X (3,4)) to consider the numer of samples (4 for X) in matrx multiplication and becomes important to determine shape of weight matrix (number of rows (4, desire neuron))\n",
    "        #and size of samples (batch size for X) are used in only calcuation but not impotant in determing shape of weight,\n",
    "        #Where n_inputs is structure of input (4 nurons) or previous layer (last output), n_neurons is samples (we can say batch size) which is desire output nurons\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)     # 0.1 to ensure that the range of values is low ( less than 1)\n",
    "        self.biases = np.zeros((1,n_neurons))   # biases we will be added to output of sum of weights so should be matrix(1,N )\n",
    "        \n",
    "    def forward(self,inputs):  # inputs is last ouput\n",
    "        self.output = np.dot(inputs,self.weights)+self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Rellu:\n",
    "    def forward(self, inputs): # inputs here is output of summation of wieghts and neurons\n",
    "        self.output = np.maximum(0,inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us to create two layers: input layer and 1st hidden layer\n",
    "\n",
    "layer1= Layer_Dense(4,5)  # takes two paramters (number of features (input neurons) here X (every sampel has 4 neurons or features) and batch size or samples)\n",
    "layer2= Layer_Dense(5,3) # we choose 5 to be like previous output neurons \n",
    "# layer3= Layer_Dense(3,3)\n",
    "\n",
    "Activation1 = Activation_Rellu()\n",
    "Activation2 = Activation_Rellu()\n",
    "\n",
    "layer1.forward(X)\n",
    "Activation1.forward(layer1.output)\n",
    "layer2.forward(Activation1.output)\n",
    "Activation2.forward(layer2.output)\n",
    "# layer3.forward(layer2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.62000104  0.29137294 -0.34475891 -0.75288288  0.24600957]\n",
      " [-0.62052457  1.08432252 -0.28839038 -0.93227544 -0.09180112]\n",
      " [-0.1593076  -0.04861024 -0.05664166 -0.66619194 -0.22422852]]\n",
      "[[0.         0.29137294 0.         0.         0.24600957]\n",
      " [0.         1.08432252 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(layer1.output)  # as Input_Matrix:X(3,4)* Weights(4,5) = output(3,5)\n",
    "# print(layer2.output)  # as Input_Matrix:last layer ouput(3,5)* Weights(5,3) = output(3,3)\n",
    "# print(layer3.output) #  (3,3) * (3,3)=(3,3)\n",
    "print(Activation1.output) # all negative values become zero becuase of Relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04184293  0.02185091  0.01518896]\n",
      " [-0.12272638  0.01665685 -0.01378473]\n",
      " [ 0.          0.          0.        ]]\n",
      "[[0.         0.02185091 0.01518896]\n",
      " [0.         0.01665685 0.        ]\n",
      " [0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(layer2.output)  # as Input_Matrix:last layer ouput(3,5)* Weights(5,3) = output(3,3)\n",
    "print(Activation2.output) # all negative values become zero becuase of Relu function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
